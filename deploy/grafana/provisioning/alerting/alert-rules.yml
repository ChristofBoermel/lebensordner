apiVersion: 1

groups:
  - orgId: 1
    name: "Lebensordner Alerts"
    folder: "Lebensordner"
    interval: 1m
    rules:
      - title: "App Down"
        uid: app-down
        condition: C
        for: 2m
        noDataState: Alerting
        execErrState: Error
        annotations:
          summary: "Lebensordner app is not responding to Prometheus scrapes"
        labels:
          severity: critical
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              expr: 'up{job="lebensordner-app"} == bool 0'
              refId: A
          - refId: B
            datasourceUid: __expr__
            model:
              type: reduce
              reducer: last
              expression: A
              refId: B
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              refId: C
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 0

      - title: "Worker Down"
        uid: worker-down
        condition: C
        for: 0s
        noDataState: OK
        execErrState: Error
        annotations:
          summary: "Lebensordner worker container has produced no logs in the last 5 minutes"
        labels:
          severity: critical
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: loki
            model:
              expr: 'absent_over_time({container_name="lebensordner-worker"}[5m])'
              refId: A
          - refId: B
            datasourceUid: __expr__
            model:
              type: reduce
              reducer: last
              expression: A
              refId: B
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              refId: C
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 0

      - title: "DB Down"
        uid: db-down
        condition: C
        for: 2m
        noDataState: Alerting
        execErrState: Error
        annotations:
          summary: "PostgreSQL database is unreachable according to postgres-exporter"
        labels:
          severity: critical
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              expr: "pg_up == bool 0"
              refId: A
          - refId: B
            datasourceUid: __expr__
            model:
              type: reduce
              reducer: last
              expression: A
              refId: B
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              refId: C
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 0

      - title: "TLS Expiry"
        uid: tls-expiry
        condition: C
        for: 0s
        noDataState: OK
        execErrState: Error
        annotations:
          summary: "A Caddy-managed TLS certificate expires in less than 14 days"
        labels:
          severity: warning
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              expr: "(caddy_tls_managed_certificate_expiry_seconds - time()) < bool 1209600"
              refId: A
          - refId: B
            datasourceUid: __expr__
            model:
              type: reduce
              reducer: last
              expression: A
              refId: B
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              refId: C
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 0

      - title: "Error Spike"
        uid: error-spike
        condition: C
        for: 0s
        noDataState: NoData
        execErrState: Error
        annotations:
          summary: "More than 3 application errors detected in the last 5 minutes"
        labels:
          severity: warning
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: loki
            model:
              expr: 'sum(count_over_time({container=~"lebensordner-app|lebensordner-worker"} | json | level="error" [5m]))'
              refId: A
          - refId: B
            datasourceUid: __expr__
            model:
              type: reduce
              reducer: last
              expression: A
              refId: B
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              refId: C
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 3
        notification_settings:
          receiver: webhook-handler
          group_wait: "0s"
          group_interval: "30m"
          repeat_interval: "30m"
